# Bluesky Scraper Configuration Instructions

## Editing Keywords (data/keywords.txt)

The keywords.txt file contains one keyword per line. These are the topics the scraper will search for.

Default keywords focus on wealth inequality:
- food insecurity
- food stamps  
- housing crisis
- eviction
- unemployment
- job loss
- pay gap
- gender inequality

To customize:
1. Open data/keywords.txt in any text editor
2. Add/remove/modify keywords (one per line)
3. Use specific phrases that people actually use in posts
4. Avoid overly broad terms that might return irrelevant results

## Editing Regex Patterns (data/regex_patterns.txt)

The regex_patterns.txt file contains filtering patterns for each keyword to ensure relevance. This follows best practices by keeping configuration in text files for easy editing.

Format: `keyword_name|pattern1|pattern2|pattern3|...`

Example:
```
food insecurity|\bfood\s*insecurit(y|ies)\b|\bhungry\b.*\bfamil(y|ies)\b|\bfood\s*desert\b
housing crisis|\bhousing\s*crisis\b|\baffordable\s*housing\b|\brent\s*crisis\b
```

To customize:
1. Open data/regex_patterns.txt in any text editor
2. Add new keywords with their patterns
3. Modify existing patterns to be more/less strict
4. Lines starting with # are comments and ignored
5. Use regex syntax - test patterns before deploying

## Runtime Configuration Knobs

### Basic Usage
```bash
python script/bsky_scraper.py
```

### Common Options

**Custom keywords (bypass keywords.txt):**
```bash
python script/bsky_scraper.py --keywords "rent crisis" "job loss" "food stamps"
```

**Exhaustive mode (recommended for comprehensive coverage):**
```bash
python script/bsky_scraper.py --time_budget_seconds 300 --max_pages_per_variant 40 --per_keyword 150
```

**Quick test run:**
```bash
python script/bsky_scraper.py --per_keyword 10 --time_budget_seconds 60
```

**Adjust delays if rate-limited:**
```bash
python script/bsky_scraper.py --delay_min 2.0 --delay_max 4.0
```

**Disable profile lookups for speed:**
```bash
python script/bsky_scraper.py --disable_profile_lookup
```

### All Configuration Options

- `--keywords`: Space-separated list of keywords to search
- `--keywords_file`: Path to keywords file (default: data/keywords.txt)  
- `--out_dir`: Output directory (default: data/out)
- `--per_keyword`: Max posts per keyword (default: 100)
- `--time_budget_seconds`: Total time budget in seconds (default: 300)
- `--max_pages_per_variant`: Max pages per query variant (default: 40)
- `--delay_min`: Minimum delay between requests in seconds (default: 1.0)
- `--delay_max`: Maximum delay between requests in seconds (default: 3.0)
- `--user_agent`: Custom user agent string
- `--disable_profile_lookup`: Skip profile location lookups for faster execution

### Time Budget Guidelines

- **Test runs**: 60-120 seconds
- **Regular collection**: 300-600 seconds (5-10 minutes)
- **Comprehensive runs**: 900-1800 seconds (15-30 minutes)

The scraper automatically distributes time fairly across keywords and stops gracefully when the budget is reached.

### Rate Limiting

If you encounter frequent rate limits:
1. Increase delays: `--delay_min 2.0 --delay_max 5.0`
2. Reduce max pages: `--max_pages_per_variant 20`
3. Disable profile lookups: `--disable_profile_lookup`

### Output Files

For each keyword, two files are generated in the output directory:
- `bluesky_{keyword}_{timestamp}.json` - Pretty-formatted JSON
- `bluesky_{keyword}_{timestamp}.csv` - Excel-compatible CSV

Files include all scraped posts with standardized fields including optional location data.

### Location Enrichment

The scraper attempts to extract location information through:
1. **Text analysis**: Looks for location mentions in post content
2. **Geo-hashtags**: Identifies location-based hashtags (#NYC, #California, etc.)  
3. **Profile lookup**: Extracts location hints from user profiles (optional)

Location fields are best-effort and may be empty if no location data is found.

### Exhaustive Coverage Strategy

The scraper uses deterministic query variants for comprehensive coverage:
- Exact phrases ("housing crisis")
- Unquoted phrases (housing crisis)
- Hashtag variants (#housingcrisis, #housing_crisis)
- Spacing variants (paygap, pay-gap, pay gap)
- Synonyms and related terms (layoff, laid off for "job loss")

Each variant is paginated deeply using round-robin allocation to avoid bias toward any single variant.

### Monitoring Progress

The scraper provides real-time progress updates:
- Keywords being processed
- Variants generated per keyword  
- Pages fetched and posts accepted per variant
- Time allocation and remaining budget
- Final summary with post counts and file locations
