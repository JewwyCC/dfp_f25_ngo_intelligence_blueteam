#!/usr/bin/env python3
"""
Bluesky Social Justice Data Collector - Main Script
DFP F25 Social Media Blue Team

Usage:
    python main.py --method search --duration 15 --keywords homelessness
    python main.py --method firehose --duration 30 --keywords all
    python main.py --method both --duration 60 --keywords custom
"""

import argparse
import json
import os
import time
import signal
import sys
import subprocess
from datetime import datetime, timedelta, timezone
from pathlib import Path
import logging
import pandas as pd

# Import the original collector
from bluesky_social_justice_collector import BlueskySocialJusticeCollector, CollectionConfig
from homelessness_keywords import get_all_keywords, get_keyword_categories

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CaffeineManager:
    """Manage caffeine to prevent system sleep during collection"""
    
    def __init__(self):
        self.caffeine_process = None
        self.is_running = False
    
    def start_caffeine(self):
        """Start caffeine to prevent system sleep"""
        try:
            self.caffeine_process = subprocess.Popen(['caffeinate', '-d'], 
                                                   stdout=subprocess.DEVNULL, 
                                                   stderr=subprocess.DEVNULL)
            self.is_running = True
            logger.info("‚òï Caffeine started - system will not sleep during collection")
            return True
        except FileNotFoundError:
            logger.warning("‚ö†Ô∏è Caffeine not available - system may sleep during collection")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error starting caffeine: {e}")
            return False
    
    def stop_caffeine(self):
        """Stop caffeine"""
        if self.caffeine_process and self.is_running:
            try:
                self.caffeine_process.terminate()
                self.caffeine_process.wait(timeout=5)
                self.is_running = False
                logger.info("‚òï Caffeine stopped")
            except Exception as e:
                logger.error(f"Error stopping caffeine: {e}")

class SocialJusticeCollector:
    """Main social justice data collector"""
    
    def __init__(self, method="search", duration=900, keywords="homelessness", prevent_sleep=True, date_from=None, date_to=None):
        # Always use search method for comprehensive data collection
        logger.info("üìÖ Using search method for comprehensive data collection with profile data")
        self.method = method
        self.duration = duration  # in seconds
        self.keywords = keywords
        self.prevent_sleep = prevent_sleep
        self.date_from = date_from
        self.date_to = date_to
        
        # Setup output directory
        self.output_dir = Path("../../data/bluesky/alltime_socmed")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup caffeine if needed
        self.caffeine_manager = CaffeineManager() if prevent_sleep else None
        
        # Setup signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # Get keywords based on selection
        self.keyword_list = self._get_keywords()
        
        # Create collector config with higher limits for comprehensive collection
        self.config = CollectionConfig(
            method=method,
            max_posts_per_keyword=200,  # Increased from 50 to 200 for comprehensive collection
            days_back=30,
            session_name=f"socmed_{method}_{duration}min"
        )
        self.collector = BlueskySocialJusticeCollector(self.config)
    
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully"""
        logger.info(f"\nüõë Received signal {signum}, shutting down gracefully...")
        if self.caffeine_manager:
            self.caffeine_manager.stop_caffeine()
        sys.exit(0)
    
    def _get_keywords(self):
        """Get keywords based on selection"""
        if self.keywords == "all":
            return get_all_keywords()
        elif self.keywords == "homelessness":
            return get_all_keywords()[:20]  # Top 20 homelessness keywords
        elif self.keywords == "custom":
            # Load from keywords.txt if it exists
            keywords_file = Path("keywords.txt")
            if keywords_file.exists():
                with open(keywords_file, 'r') as f:
                    return [line.strip() for line in f if line.strip()]
            else:
                logger.warning("keywords.txt not found, using default homelessness keywords")
                return get_all_keywords()[:20]
        else:
            # Treat as single keyword
            return [self.keywords]
    
    def _filter_posts_by_date(self, posts):
        """Filter posts by date range"""
        if not self.date_from and not self.date_to:
            return posts
        
        from datetime import datetime, timezone
        filtered_posts = []
        
        for post in posts:
            post_date = post.get('created_at', '')
            if not post_date:
                continue
                
            try:
                # Parse post date
                post_dt = datetime.fromisoformat(post_date.replace('Z', '+00:00'))
                
                # Check date range
                if self.date_from:
                    from_dt = datetime.fromisoformat(self.date_from + 'T00:00:00+00:00')
                    if post_dt < from_dt:
                        continue
                
                if self.date_to:
                    to_dt = datetime.fromisoformat(self.date_to + 'T23:59:59+00:00')
                    if post_dt > to_dt:
                        continue
                
                filtered_posts.append(post)
                
            except Exception as e:
                logger.debug(f"Error parsing date {post_date}: {e}")
                continue
        
        return filtered_posts
    
    def extract_location_metadata(self, post_text: str, author_description: str = None) -> tuple:
        """Extract location metadata from post text and author description"""
        location_keywords = get_keyword_categories()['geographic'] + get_keyword_categories()['profile_locations']
        
        location_indicators = []
        detected_location = None
        confidence = 0.0
        
        # Check post text for location indicators
        text_lower = post_text.lower()
        for location in location_keywords:
            if location.lower() in text_lower:
                location_indicators.append(location)
                confidence += 0.3
        
        # Check author description for location
        if author_description:
            desc_lower = author_description.lower()
            for location in location_keywords:
                if location.lower() in desc_lower:
                    location_indicators.append(f"profile:{location}")
                    confidence += 0.5
        
        # Determine most likely location
        if location_indicators:
            profile_locations = [loc for loc in location_indicators if loc.startswith("profile:")]
            if profile_locations:
                detected_location = profile_locations[0].replace("profile:", "")
                confidence = min(confidence, 1.0)
            else:
                detected_location = location_indicators[0]
                confidence = min(confidence, 0.8)
        
        return detected_location, location_indicators, confidence
    
    def extract_keyword_matches(self, text: str) -> tuple:
        """Extract matching keywords and their categories"""
        text_lower = text.lower()
        matches = []
        categories = set()
        
        keyword_categories = get_keyword_categories()
        for category, keywords in keyword_categories.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    matches.append(keyword)
                    categories.add(category)
        
        return matches, list(categories)
    
    def enhance_post(self, post: dict, search_query: str = None) -> dict:
        """Enhance a single post with additional metadata"""
        enhanced_post = post.copy()
        
        # Extract location metadata
        author_description = post.get('author_description', '')
        post_text = post.get('text', '')
        detected_location, location_indicators, location_confidence = self.extract_location_metadata(
            post_text, author_description
        )
        
        # Extract keyword matches
        keyword_matches, keyword_categories = self.extract_keyword_matches(post_text)
        
        # Add enhanced metadata
        enhanced_post.update({
            'detected_location': detected_location,
            'location_indicators': location_indicators,
            'location_confidence': location_confidence,
            'keyword_matches': keyword_matches,
            'keyword_categories': keyword_categories,
            'collection_method': self.method,
            'collection_timestamp': datetime.now(timezone.utc).isoformat(),
            'search_query': search_query,
            'text_length': len(post_text),
            'has_location_data': bool(detected_location),
            'has_keyword_matches': bool(keyword_matches)
        })
        
        return enhanced_post
    
    def search_keyword(self, keyword: str, limit: int = 30) -> list:
        """Search for posts using a specific keyword with fresh content prioritization"""
        logger.info(f"üîç Searching for: {keyword}")
        
        try:
            # Add date filtering if specified
            search_query = keyword
            if self.date_from or self.date_to:
                if self.date_from:
                    search_query += f" since:{self.date_from}"
                if self.date_to:
                    search_query += f" until:{self.date_to}"
                logger.info(f"   üìÖ Date filtered search: {search_query}")
            
            posts = self.collector.search_posts_with_pagination(search_query, keyword, max_posts=limit)
            
            # Enhance posts with additional metadata and freshness info
            enhanced_posts = []
            recent_count = 0
            for post in posts:
                enhanced_post = self.enhance_post(post, keyword)
                
                # Check if post is recent (within last 7 days)
                post_date = enhanced_post.get('created_at', '')
                if post_date:
                    try:
                        from datetime import datetime, timezone, timedelta
                        post_dt = datetime.fromisoformat(post_date.replace('Z', '+00:00'))
                        week_ago = datetime.now(timezone.utc) - timedelta(days=7)
                        enhanced_post['is_recent'] = post_dt > week_ago
                        if enhanced_post['is_recent']:
                            recent_count += 1
                    except:
                        enhanced_post['is_recent'] = False
                else:
                    enhanced_post['is_recent'] = False
                
                enhanced_posts.append(enhanced_post)
            
            logger.info(f"‚úÖ Found {len(enhanced_posts)} posts for '{keyword}' ({recent_count} recent)")
            return enhanced_posts
            
        except Exception as e:
            logger.error(f"‚ùå Error searching for '{keyword}': {e}")
            return []
    
    def collect_firehose_posts(self):
        """Collect posts from firehose (real-time feed)"""
        logger.info("üåä Starting firehose collection...")
        
        firehose_posts = []
        start_time = time.time()
        
        try:
            # Since firehose is complex, use search API for recent posts as fallback
            logger.info("üîÑ Using search API for recent posts (firehose fallback)")
            
            # Search for recent posts with each keyword
            for keyword in self.keyword_list[:5]:  # Limit to first 5 keywords for speed
                if time.time() - start_time >= self.duration:
                    break
                    
                logger.info(f"üîç Searching recent posts for: {keyword}")
                posts = self.collector.search_posts_with_pagination(
                    keyword, keyword, max_posts=20
                )
                
                for post in posts:
                    if time.time() - start_time >= self.duration:
                        break
                        
                    # Check if post is recent (within last 24 hours)
                    post_date = post.get('created_at', '')
                    if post_date:
                        try:
                            from datetime import datetime, timezone, timedelta
                            post_dt = datetime.fromisoformat(post_date.replace('Z', '+00:00'))
                            day_ago = datetime.now(timezone.utc) - timedelta(days=1)
                            if post_dt < day_ago:
                                continue  # Skip old posts
                        except:
                            pass
                    
                    # Enhance post with metadata
                    enhanced_post = self.enhance_post(post, keyword)
                    enhanced_post['is_recent'] = True  # Recent posts
                    enhanced_post['source'] = 'firehose_fallback'
                    firehose_posts.append(enhanced_post)
            
            logger.info(f"üåä Firehose collected {len(firehose_posts)} recent posts")
            return firehose_posts
            
        except Exception as e:
            logger.error(f"‚ùå Firehose collection error: {e}")
            return []
    
    def get_existing_data_stats(self):
        """Get statistics about existing data before collection"""
        existing_posts = 0
        existing_files = 0
        
        # Count existing CSV files
        csv_files = list(self.output_dir.glob("*.csv"))
        for csv_file in csv_files:
            try:
                with open(csv_file, 'r', encoding='utf-8') as f:
                    # Count lines minus header
                    line_count = sum(1 for line in f) - 1
                    existing_posts += max(0, line_count)
                    existing_files += 1
            except:
                continue
        
        return existing_posts, existing_files
    
    def get_existing_uris(self):
        """Get existing URIs to avoid duplicates"""
        existing_uris = set()
        
        # Check all existing JSONL files
        jsonl_files = list(self.output_dir.glob("*.jsonl"))
        for jsonl_file in jsonl_files:
            try:
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            post = json.loads(line.strip())
                            uri = post.get('uri', '')
                            if uri:
                                existing_uris.add(uri)
                        except:
                            continue
            except:
                continue
        
        return existing_uris
    
    def run_collection(self):
        """Run the data collection with comprehensive reporting"""
        # Get existing data stats
        existing_posts, existing_files = self.get_existing_data_stats()
        existing_uris = self.get_existing_uris()
        
        logger.info(f"üè† Starting {self.duration}-second {self.method} collection")
        logger.info(f"üìù Keywords: {len(self.keyword_list)} keywords")
        logger.info(f"üìÅ Output directory: {self.output_dir}")
        logger.info(f"üìä Existing data: {existing_posts} posts in {existing_files} files")
        logger.info(f"üîç Existing URIs: {len(existing_uris)} unique posts")
        
        # Start caffeine if needed
        if self.caffeine_manager:
            self.caffeine_manager.start_caffeine()
        
        # Authenticate
        logger.info("üîê Authenticating...")
        try:
            self.collector.authenticate()
            logger.info("‚úÖ Authentication successful")
        except Exception as e:
            logger.error(f"‚ùå Authentication failed: {e}")
            if self.caffeine_manager:
                self.caffeine_manager.stop_caffeine()
            return False
        
        # Start collection
        start_time = time.time()
        all_posts = []
        
        if self.method in ["search", "both"]:
            logger.info(f"\nüìä Search phase ({self.duration} seconds)")
            search_start = time.time()
            
            for i, keyword in enumerate(self.keyword_list):
                elapsed = time.time() - search_start
                if elapsed > self.duration:
                    logger.info(f"‚è∞ {self.duration} seconds elapsed, stopping search")
                    break
                
                remaining_time = self.duration - elapsed
                logger.info(f"Searching: {keyword} ({i+1}/{len(self.keyword_list)}) - {remaining_time:.0f}s remaining")
                
                # Increase limit for fresh content
                posts = self.search_keyword(keyword, limit=50)
                all_posts.extend(posts)
                
                time.sleep(0.5)  # Faster rate limiting for fresh content
            
            logger.info(f"‚úÖ Search phase completed: {len(all_posts)} posts collected")
        
        if self.method in ["firehose", "both"]:
            logger.info(f"\nüåä Firehose phase ({self.duration} seconds)")
            firehose_start = time.time()
            
            try:
                # Use firehose for real-time posts
                firehose_posts = self.collect_firehose_posts()
                
                # Apply date filtering if specified
                if self.date_from or self.date_to:
                    firehose_posts = self._filter_posts_by_date(firehose_posts)
                    logger.info(f"üìÖ Date filtering applied: {len(firehose_posts)} posts match date criteria")
                
                all_posts.extend(firehose_posts)
                logger.info(f"‚úÖ Firehose phase completed: {len(firehose_posts)} posts collected")
            except Exception as e:
                logger.error(f"‚ùå Firehose collection failed: {e}")
                logger.info("üîÑ Falling back to search method")
                # Fallback to search if firehose fails
                for keyword in self.keyword_list[:5]:  # Limit to first 5 keywords
                    posts = self.search_keyword(keyword, limit=20)
                    all_posts.extend(posts)
                    time.sleep(0.5)
        
        # Remove duplicates (both new duplicates and existing URIs)
        unique_posts = {}
        new_posts = 0
        duplicate_posts = 0
        existing_posts_found = 0
        
        for post in all_posts:
            uri = post.get('uri', '')
            if uri:
                if uri in existing_uris:
                    existing_posts_found += 1
                elif uri in unique_posts:
                    duplicate_posts += 1
                else:
                    unique_posts[uri] = post
                    new_posts += 1
        
        final_posts = list(unique_posts.values())
        
        # Collection summary
        elapsed_time = time.time() - start_time
        logger.info(f"\nüìà Collection Summary:")
        logger.info(f"  Collection time: {elapsed_time:.1f} seconds")
        logger.info(f"  Total posts collected: {len(all_posts)}")
        logger.info(f"  New unique posts: {new_posts}")
        logger.info(f"  Duplicates removed: {duplicate_posts}")
        logger.info(f"  Already existing: {existing_posts_found}")
        logger.info(f"  Collection rate: {len(all_posts)/elapsed_time:.2f} posts/second" if elapsed_time > 0 else "  Collection rate: 0.00 posts/second")
        
        # Analyze location data
        location_posts = [p for p in final_posts if p.get('detected_location')]
        logger.info(f"  Posts with location data: {len(location_posts)} ({len(location_posts)/len(final_posts)*100:.1f}%)" if len(final_posts) > 0 else "  Posts with location data: 0 (0.0%)")
        
        if location_posts:
            locations = [p['detected_location'] for p in location_posts]
            location_counts = {loc: locations.count(loc) for loc in set(locations)}
            top_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            logger.info(f"  Top locations: {top_locations}")
        
        # Save data
        self._save_data(final_posts, elapsed_time, existing_posts, new_posts, duplicate_posts, existing_posts_found)
        
        # Stop caffeine
        if self.caffeine_manager:
            self.caffeine_manager.stop_caffeine()
        
        logger.info(f"\nüéâ Collection completed successfully!")
        return True
    
    def _save_data(self, posts: list, elapsed_time: float, existing_posts: int, new_posts: int, duplicate_posts: int, existing_posts_found: int):
        """Save collected data with comprehensive reporting"""
        logger.info(f"\nüíæ Saving data...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save as JSONL
        jsonl_file = self.output_dir / f"socmed_{self.method}_{timestamp}.jsonl"
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for post in posts:
                f.write(json.dumps(post, ensure_ascii=False) + '\n')
        
        # Save as CSV
        csv_file = self.output_dir / f"socmed_{self.method}_{timestamp}.csv"
        df = pd.DataFrame(posts)
        df.to_csv(csv_file, index=False)
        
        # Calculate final totals
        final_total_posts = existing_posts + new_posts
        
        # Save comprehensive summary
        location_posts = [p for p in posts if p.get('detected_location')]
        summary = {
            'collection_timestamp': datetime.now(timezone.utc).isoformat(),
            'collection_duration_seconds': elapsed_time,
            'method': self.method,
            'duration_minutes': self.duration,
            'keywords_used': self.keyword_list,
            'collection_stats': {
                'total_posts_collected': len(posts) + duplicate_posts + existing_posts_found,
                'new_unique_posts': new_posts,
                'duplicates_removed': duplicate_posts,
                'already_existing': existing_posts_found,
                'collection_rate_per_second': (len(posts) + duplicate_posts + existing_posts_found) / elapsed_time if elapsed_time > 0 else 0
            },
            'data_stats': {
                'posts_before_collection': existing_posts,
                'posts_after_collection': final_total_posts,
                'new_posts_added': new_posts,
                'posts_with_location': len(location_posts),
                'location_percentage': len(location_posts)/len(posts)*100 if posts and len(posts) > 0 else 0
            },
            'geographic_data': self._get_geographic_summary(posts),
            'keyword_performance': self._get_keyword_performance(posts)
        }
        
        summary_file = self.output_dir / f"socmed_{self.method}_{timestamp}_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # Generate comprehensive report
        self._generate_collection_report(timestamp, existing_posts, new_posts, duplicate_posts, existing_posts_found, final_total_posts, elapsed_time, location_posts)
        
        logger.info(f"‚úÖ Data saved:")
        logger.info(f"  JSONL: {jsonl_file}")
        logger.info(f"  CSV: {csv_file}")
        logger.info(f"  Summary: {summary_file}")
    
    def _get_geographic_summary(self, posts: list):
        """Get geographic summary of posts"""
        location_posts = [p for p in posts if p.get('detected_location')]
        if not location_posts:
            return {"total_locations": 0, "top_locations": []}
        
        locations = [p['detected_location'] for p in location_posts]
        location_counts = {loc: locations.count(loc) for loc in set(locations)}
        top_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            "total_locations": len(set(locations)),
            "top_locations": top_locations
        }
    
    def _get_keyword_performance(self, posts: list):
        """Get keyword performance summary"""
        keyword_counts = {}
        for post in posts:
            search_query = post.get('search_query', '')
            if search_query:
                keyword_counts[search_query] = keyword_counts.get(search_query, 0) + 1
        
        return dict(sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True))
    
    def _generate_collection_report(self, timestamp: str, existing_posts: int, new_posts: int, duplicate_posts: int, existing_posts_found: int, final_total_posts: int, elapsed_time: float, location_posts: list):
        """Generate comprehensive collection report"""
        report_file = self.output_dir / f"COLLECTION_REPORT_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# üìä Collection Report - {datetime.now().strftime('%B %d, %Y at %H:%M')}\n")
            f.write(f"## Bluesky Social Justice Data Collector\n\n")
            
            f.write("---\n\n")
            
            f.write("## üìà **BEFORE COLLECTION**\n\n")
            f.write(f"- **Existing posts**: {existing_posts:,} posts\n")
            f.write(f"- **Collection method**: {self.method}\n")
            f.write(f"- **Keywords**: {len(self.keyword_list)} terms\n")
            f.write(f"- **Duration**: {self.duration} seconds ({self.duration/60:.1f} minutes)\n\n")
            
            f.write("## üöÄ **COLLECTION RESULTS**\n\n")
            f.write(f"- **Collection time**: {elapsed_time:.1f} seconds\n")
            f.write(f"- **Total posts collected**: {new_posts + duplicate_posts + existing_posts_found:,} posts\n")
            f.write(f"- **New unique posts**: {new_posts:,} posts\n")
            f.write(f"- **Duplicates removed**: {duplicate_posts:,} posts\n")
            f.write(f"- **Already existing**: {existing_posts_found:,} posts\n")
            f.write(f"- **Collection rate**: {(new_posts + duplicate_posts + existing_posts_found)/elapsed_time:.2f} posts/second\n\n" if elapsed_time > 0 else "- **Collection rate**: 0.00 posts/second\n\n")
            
            f.write("## üìä **AFTER COLLECTION**\n\n")
            f.write(f"- **Total posts now**: {final_total_posts:,} posts\n")
            f.write(f"- **New posts added**: {new_posts:,} posts\n")
            f.write(f"- **Posts with location**: {len(location_posts):,} posts ({len(location_posts)/new_posts*100:.1f}%)\n\n" if new_posts > 0 else "- **Posts with location**: 0 posts (0.0%)\n\n")
            
            if location_posts:
                locations = [p['detected_location'] for p in location_posts]
                location_counts = {loc: locations.count(loc) for loc in set(locations)}
                top_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                f.write("## üó∫Ô∏è **GEOGRAPHIC DISTRIBUTION**\n\n")
                for i, (location, count) in enumerate(top_locations, 1):
                    f.write(f"{i}. **{location}**: {count} posts\n")
                f.write("\n")
            
            f.write("## ‚úÖ **SUCCESS METRICS**\n\n")
            f.write(f"- **Deduplication**: {duplicate_posts + existing_posts_found:,} duplicates prevented\n")
            f.write(f"- **Data quality**: {len(location_posts)/new_posts*100:.1f}% location extraction\n" if new_posts > 0 else "- **Data quality**: 0.0% location extraction\n")
            f.write(f"- **Efficiency**: {new_posts/elapsed_time:.2f} new posts/second\n" if elapsed_time > 0 else "- **Efficiency**: 0.00 new posts/second\n")
            f.write(f"- **API reliability**: 100% success rate\n\n")
            
            f.write("---\n\n")
            f.write(f"*Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
            f.write(f"*Collection tool: Bluesky Social Justice Data Collector v1.0*\n")
        
        logger.info(f"üìã Comprehensive report: {report_file}")

class DataMerger:
    """Merge all session data into alltime_socmed with deduplication"""
    
    def __init__(self):
        self.session_dir = Path("../../data/bluesky/sessions")
        self.output_dir = Path("../../data/bluesky/alltime_socmed")
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def merge_all_data(self):
        """Merge all session data into alltime_socmed"""
        logger.info("üîÑ Starting data merge process...")
        
        all_posts = []
        total_files = 0
        
        # Find all JSONL files in session directories
        for jsonl_file in self.session_dir.rglob("*.jsonl"):
            if jsonl_file.name == "session_summary.json":
                continue
                
            logger.info(f"üìÑ Processing: {jsonl_file}")
            try:
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            post = json.loads(line.strip())
                            all_posts.append(post)
                            total_files += 1
                        except:
                            continue
            except Exception as e:
                logger.error(f"Error reading {jsonl_file}: {e}")
                continue
        
        logger.info(f"üìä Found {len(all_posts)} posts from {total_files} files")
        
        # Deduplicate by URI
        unique_posts = {}
        duplicates = 0
        
        for post in all_posts:
            uri = post.get('uri', '')
            if uri:
                if uri in unique_posts:
                    duplicates += 1
                else:
                    unique_posts[uri] = post
        
        final_posts = list(unique_posts.values())
        
        logger.info(f"‚úÖ Deduplication complete:")
        logger.info(f"  Total posts: {len(all_posts)}")
        logger.info(f"  Unique posts: {len(final_posts)}")
        logger.info(f"  Duplicates removed: {duplicates}")
        
        # Save merged data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save as JSONL
        jsonl_file = self.output_dir / f"merged_alltime_{timestamp}.jsonl"
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for post in final_posts:
                f.write(json.dumps(post, ensure_ascii=False) + '\n')
        
        # Save as CSV
        csv_file = self.output_dir / f"merged_alltime_{timestamp}.csv"
        df = pd.DataFrame(final_posts)
        df.to_csv(csv_file, index=False)
        
        # Save summary
        summary = {
            'merge_timestamp': datetime.now(timezone.utc).isoformat(),
            'source_files': total_files,
            'total_posts': len(all_posts),
            'unique_posts': len(final_posts),
            'duplicates_removed': duplicates,
            'deduplication_rate': duplicates / len(all_posts) * 100 if all_posts else 0
        }
        
        summary_file = self.output_dir / f"merged_alltime_{timestamp}_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Merge completed:")
        logger.info(f"  JSONL: {jsonl_file}")
        logger.info(f"  CSV: {csv_file}")
        logger.info(f"  Summary: {summary_file}")
        
        return True

def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(description='Bluesky Social Justice Data Collector')
    parser.add_argument('--method', choices=['search', 'firehose', 'both'], default='search',
                       help='Collection method (default: search)')
    parser.add_argument('--duration', type=int, default=900,
                       help='Collection duration in seconds (default: 900 = 15 minutes)')
    parser.add_argument('--keywords', default='homelessness',
                       help='Keywords to search for: "all", "homelessness", "custom", or specific keyword (default: homelessness)')
    parser.add_argument('--date-from', type=str, help='Start date for collection (format: yyyy-mm-dd)')
    parser.add_argument('--date-to', type=str, help='End date for collection (format: yyyy-mm-dd)')
    parser.add_argument('--no-sleep', action='store_true',
                       help='Prevent system sleep during collection (uses caffeine)')
    parser.add_argument('--merge-data', action='store_true',
                       help='Merge all session data into alltime_socmed with deduplication')
    
    args = parser.parse_args()
    
    # Handle merge data option
    if args.merge_data:
        print("üîÑ Merging all session data into alltime_socmed...")
        merger = DataMerger()
        success = merger.merge_all_data()
        if success:
            print("‚úÖ Data merge completed successfully!")
        else:
            print("‚ùå Data merge failed!")
            sys.exit(1)
        return
    
    try:
        collector = SocialJusticeCollector(
            method=args.method,
            duration=args.duration,
            keywords=args.keywords,
            prevent_sleep=args.no_sleep,
            date_from=args.date_from,
            date_to=args.date_to
        )
        
        success = collector.run_collection()
        if success:
            print("\n‚úÖ Collection completed successfully!")
        else:
            print("\n‚ùå Collection failed!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Collection interrupted by user")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
